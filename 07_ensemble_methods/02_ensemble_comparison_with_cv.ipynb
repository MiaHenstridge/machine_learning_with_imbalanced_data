{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0f3f8a",
   "metadata": {},
   "source": [
    "# Comparing over-sampling methods using cross-validation and hyperparameter tuning\n",
    "Machine Learning with Imbalanced Data - Course\n",
    "\n",
    "When we train a classifier, we want it to predict an outcome in a real life dataset. Thus, it is important to evaluate the performance of the classifier on a data set with the original distribution of classes, and not on the rebalanced data.\n",
    "\n",
    "This means, that the over-sampling methods should be performed on the dataset that we are going to use to train the classifier. But, **the performance of the model should be determined on a portion of the data, that was not re-sampled.**\n",
    "\n",
    "In this notebook, we will use the imbalanced-learn pipeline, to set up various over-sampling solutions in a way that we train the model on re-sampled data, but we evaluate performance on non-resampled data.\n",
    "\n",
    "In addition, we will optimize the hyperparamters of the random forests, so we ensure we have the best possible model, trained on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ec9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, RUSBoostClassifier, EasyEnsembleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496bb73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_ls = [\n",
    "    'ecoli',\n",
    "    'optical_digits',\n",
    "    'satimage',\n",
    "    'pen_digits',\n",
    "    'abalone',\n",
    "    'sick_euthyroid',\n",
    "    'spectrometer',\n",
    "    'car_eval_34',\n",
    "    'isolet',\n",
    "    'us_crime',\n",
    "    'yeast_ml8',\n",
    "    'scene',\n",
    "    'libras_move',\n",
    "    'thyroid_sick',\n",
    "    'coil_2000',\n",
    "    'arrhythmia',\n",
    "    'solar_flare_m0',\n",
    "    'oil',\n",
    "    'car_eval_4',\n",
    "    'wine_quality',\n",
    "    'letter_img',\n",
    "    'yeast_me2',\n",
    "    'webpage',\n",
    "    'ozone_level',\n",
    "    'mammography',\n",
    "    'protein_homo',\n",
    "    'abalone_19',\n",
    "]\n",
    "\n",
    "len(datasets_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b537d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble methods (with or without resampling)\n",
    "\n",
    "ensemble_dict = {\n",
    "    \n",
    "    # just bagging\n",
    "    'random_forests': RandomForestClassifier(random_state=39),     \n",
    "    \n",
    "    # just boosting\n",
    "    'adaboost' : AdaBoostClassifier(\n",
    "        algorithm=\"SAMME\",\n",
    "        random_state=2909,\n",
    "    ),\n",
    "    \n",
    "    # stronger boosting algo\n",
    "    'gbm' : GradientBoostingClassifier(random_state=39),\n",
    "\n",
    "    # balanced random forests (balanced bagging)\n",
    "    'balancedRF': BalancedRandomForestClassifier(\n",
    "        criterion='gini',\n",
    "        sampling_strategy='auto',\n",
    "        random_state=2909,\n",
    "        replacement=False,\n",
    "        bootstrap=True,\n",
    "    ),\n",
    "\n",
    "    # bagging of adaboost, no resampling\n",
    "    'bagging': BaggingClassifier(\n",
    "        # estimator=ada, we'll optimize this parameter\n",
    "        n_estimators=20,\n",
    "        n_jobs=4,\n",
    "        random_state=2909,\n",
    "    ),\n",
    "\n",
    "    # boosting + undersampling\n",
    "    'rusboost': RUSBoostClassifier(\n",
    "        estimator=None,\n",
    "        n_estimators=20,\n",
    "        learning_rate=1.0,\n",
    "        sampling_strategy='auto',\n",
    "        random_state=2909,\n",
    "    ),\n",
    "\n",
    "    # bagging + boosting + under-sammpling\n",
    "    'easyEnsemble': EasyEnsembleClassifier(\n",
    "        n_estimators=20,\n",
    "        sampling_strategy='auto',\n",
    "        n_jobs=4,\n",
    "        random_state=2909,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aab8e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaboost with different tree sizes for bagging\n",
    "# without resampling\n",
    "\n",
    "ada_ls = [\n",
    "    AdaBoostClassifier(\n",
    "        n_estimators=5,\n",
    "        algorithm=\"SAMME\",\n",
    "        random_state=2909,\n",
    "    ),\n",
    "    \n",
    "    AdaBoostClassifier(\n",
    "        n_estimators=10,\n",
    "        algorithm=\"SAMME\",\n",
    "        random_state=2909,\n",
    "    ),\n",
    "    \n",
    "    AdaBoostClassifier(\n",
    "        n_estimators=20,\n",
    "        algorithm=\"SAMME\",\n",
    "        random_state=2909,\n",
    "    ),\n",
    "    \n",
    "\n",
    "    AdaBoostClassifier(\n",
    "        n_estimators=40,\n",
    "        algorithm=\"SAMME\",\n",
    "        random_state=2909,\n",
    "    ),   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf33fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nestimators_params = {\"n_estimators\": [5, 10, 20, 40]}\n",
    "\n",
    "nestimators_depth_params = {\n",
    "    \"n_estimators\": [10, 50, 100, 500],\n",
    "    \"max_depth\": [1, 2, 3, 4],\n",
    "}\n",
    "\n",
    "bagged_ada_params = {\n",
    "    \"n_estimators\": [10, 50, 100, 500],\n",
    "    \"estimator\": ada_ls,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we train several models, with the different ensembles and\n",
    "# with cross-validation for each dataset\n",
    "\n",
    "# to store the results\n",
    "mean_dict = {}\n",
    "std_dict = {}\n",
    "model_dict = {}\n",
    "\n",
    "\n",
    "for dataset in datasets_ls:\n",
    "\n",
    "    # initiate a dictionary per dataset\n",
    "    mean_dict[dataset] = {}\n",
    "    std_dict[dataset] = {}\n",
    "    model_dict[dataset] = {}\n",
    "\n",
    "    print(dataset)\n",
    "\n",
    "    # load dataset\n",
    "    data = fetch_datasets()[dataset]\n",
    "\n",
    "    # separate dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.data,\n",
    "        data.target,\n",
    "        test_size=0.3,\n",
    "        random_state=0,\n",
    "    )\n",
    "\n",
    "    for ensemble in ensemble_dict.keys():\n",
    "        \n",
    "        print(ensemble)\n",
    "        \n",
    "        model = ensemble_dict[ensemble]\n",
    "        \n",
    "        if ensemble in [\"random_forests\", \"gbm\"]:\n",
    "            params = nestimators_depth_params\n",
    "        elif ensemble in [\"bagging\"]:\n",
    "            params = bagged_ada_params\n",
    "        else:\n",
    "            params = nestimators_params\n",
    "\n",
    "        # train and evaluate performance\n",
    "        # with cross-validation\n",
    "        search = GridSearchCV(\n",
    "            model,\n",
    "            params,\n",
    "            scoring=\"roc_auc\",\n",
    "        )\n",
    "\n",
    "        search.fit(X_train, y_train)\n",
    "\n",
    "        print('Best parameters: {0}, \\n Best score: {1}'.format(\n",
    "            search.best_params_, search.best_score_))\n",
    "\n",
    "        m, s = pd.DataFrame(search.cv_results_).sort_values(\n",
    "            by=\"mean_test_score\",\n",
    "            ascending=False).loc[0, [\"mean_test_score\", \"std_test_score\"]].values\n",
    "\n",
    "        # store results\n",
    "        mean_dict[dataset][ensemble] = m\n",
    "        std_dict[dataset][ensemble] = s\n",
    "        model_dict[dataset][ensemble] = search\n",
    "        \n",
    "        print()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we plot the performance of the models\n",
    "\n",
    "for dataset in datasets_ls:\n",
    "    \n",
    "    mean_s = pd.Series(mean_dict[dataset])\n",
    "    std_s = pd.Series(std_dict[dataset])\n",
    "    \n",
    "    mean_s.plot.bar(yerr=[std_s, std_s]\n",
    "        )\n",
    "    plt.title(dataset)\n",
    "    plt.ylabel('Average ROC-AUC')\n",
    "    plt.axhline(mean_dict[dataset]['random_forests'], color='r')\n",
    "    plt.axhline(mean_dict[dataset]['adaboost'], color='g')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309eacae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
